library(ranger)
library(patchwork)
set.seed(42)
setwd("C:/Users/user/Documents/GitHub/Data_Science")
# 1) Read data
df <- fread("healthcare-dataset-stroke-data.csv") %>% as_tibble()
# 1) Read data
df <- fread("healthcare_dataset_stroke_data.csv") %>% as_tibble()
# Quick snapshot
cat("Rows:", nrow(df), "Columns:", ncol(df), "\n")
print(names(df))
print(table(df$stroke))
# 2) Basic cleaning & type conversion
df <- df %>%
mutate(
gender = as.factor(gender),
ever_married = as.factor(ever_married),
work_type = as.factor(work_type),
Residence_type = as.factor(Residence_type),
smoking_status = as.factor(smoking_status),
hypertension = as.factor(hypertension),
heart_disease = as.factor(heart_disease),
stroke = as.factor(stroke)
)
# 3) Missingness overview
missing_table <- sapply(df, function(x) sum(is.na(x)))
print(missing_table)
# 4) Impute BMI median within group (example: median BMI overall or by gender)
# Here we use overall median (or you can use grouping)
bmi_median <- median(df$bmi, na.rm=TRUE)
df$bmi <- ifelse(is.na(df$bmi), bmi_median, df$bmi)
# 5) Feature engineering - (if needed)
# e.g., bucket age into categories (optional)
df <- df %>%
mutate(age_group = case_when(
age < 18 ~ "child",
age >= 18 & age < 35 ~ "young_adult",
age >= 35 & age < 60 ~ "adult",
age >= 60 ~ "senior"
)) %>% mutate(age_group = factor(age_group, levels=c("child","young_adult","adult","senior")))
# 6) Train-test split (stratified by stroke)
train_index <- createDataPartition(df$stroke, p = 0.8, list = FALSE)
train <- df[train_index, ]
test  <- df[-train_index, ]
# 7) Preprocessing via caret
# We'll one-hot encode categorical vars and center/scale numeric variables
prep_recipe <- preProcess(train %>% select(age, avg_glucose_level, bmi),
method = c("center","scale"))
train_num <- predict(prep_recipe, train %>% select(age, avg_glucose_level, bmi))
test_num  <- predict(prep_recipe, test %>% select(age, avg_glucose_level, bmi))
# Create model frames with dummies
dummies <- dummyVars(~ gender + ever_married + work_type + Residence_type + smoking_status + hypertension + heart_disease + age_group,
data = train)
train_cat <- predict(dummies, newdata = train) %>% as.data.frame()
test_cat  <- predict(dummies, newdata = test) %>% as.data.frame()
train_model <- bind_cols(train_num, train_cat) %>% mutate(stroke = train$stroke)
test_model  <- bind_cols(test_num, test_cat) %>% mutate(stroke = test$stroke)
# 8) Baseline model: Logistic regression
ctrl <- trainControl(method="cv", number=5, classProbs=TRUE, summaryFunction=twoClassSummary, savePredictions="final")
set.seed(42)
# caret requires the positive class as the first level sometimes; ensure factor levels:
train_model$stroke <- relevel(train_model$stroke, ref = "1") # make '1' positive if needed
test_model$stroke  <- relevel(test_model$stroke, ref = "1")
glm_fit <- train(stroke ~ ., data = train_model,
method="glm",
family="binomial",
metric="ROC",
trControl = ctrl)
# 2) Basic cleaning & type conversion
df <- fread("healthcare_dataset_stroke_data.csv") %>%
as_tibble() %>%
mutate(
gender = as.factor(gender),
ever_married = as.factor(ever_married),
work_type = as.factor(work_type),
Residence_type = as.factor(Residence_type),
smoking_status = as.factor(smoking_status),
hypertension = as.factor(hypertension),
heart_disease = as.factor(heart_disease),
stroke = factor(stroke, levels = c(0, 1),
labels = c("NoStroke", "Stroke"))
)
# 3) Missingness overview
missing_table <- sapply(df, function(x) sum(is.na(x)))
print(missing_table)
# 4) Impute BMI median within group (example: median BMI overall or by gender)
# Here we use overall median (or you can use grouping)
bmi_median <- median(df$bmi, na.rm=TRUE)
df$bmi <- ifelse(is.na(df$bmi), bmi_median, df$bmi)
# 5) Feature engineering - (if needed)
# e.g., bucket age into categories (optional)
df <- df %>%
mutate(age_group = case_when(
age < 18 ~ "child",
age >= 18 & age < 35 ~ "young_adult",
age >= 35 & age < 60 ~ "adult",
age >= 60 ~ "senior"
)) %>% mutate(age_group = factor(age_group, levels=c("child","young_adult","adult","senior")))
# 6) Train-test split (stratified by stroke)
train_index <- createDataPartition(df$stroke, p = 0.8, list = FALSE)
train <- df[train_index, ]
test  <- df[-train_index, ]
# 7) Preprocessing via caret
# We'll one-hot encode categorical vars and center/scale numeric variables
prep_recipe <- preProcess(train %>% select(age, avg_glucose_level, bmi),
method = c("center","scale"))
train_num <- predict(prep_recipe, train %>% select(age, avg_glucose_level, bmi))
test_num  <- predict(prep_recipe, test %>% select(age, avg_glucose_level, bmi))
# Create model frames with dummies
dummies <- dummyVars(~ gender + ever_married + work_type + Residence_type + smoking_status + hypertension + heart_disease + age_group,
data = train)
train_cat <- predict(dummies, newdata = train) %>% as.data.frame()
test_cat  <- predict(dummies, newdata = test) %>% as.data.frame()
train_model <- bind_cols(train_num, train_cat) %>% mutate(stroke = train$stroke)
test_model  <- bind_cols(test_num, test_cat) %>% mutate(stroke = test$stroke)
# 8) Baseline model: Logistic regression
ctrl <- trainControl(method="cv", number=5, classProbs=TRUE, summaryFunction=twoClassSummary, savePredictions="final")
set.seed(42)
# caret requires the positive class as the first level sometimes; ensure factor levels:
train_model$stroke <- relevel(train_model$stroke, ref = "1") # make '1' positive if needed
# caret requires the positive class as the first level sometimes; ensure factor levels:
train_model$stroke <- relevel(train_model$stroke, ref = "Stroke")
test_model$stroke  <- relevel(test_model$stroke,  ref = "Stroke")
glm_fit <- train(stroke ~ ., data = train_model,
method="glm",
family="binomial",
metric="ROC",
trControl = ctrl)
print(glm_fit)
# Predictions on test
glm_prob <- predict(glm_fit, newdata = test_model, type = "prob")[, "1"]
# Predictions on test
glm_prob <- predict(glm_fit, newdata = test_model, type = "prob")[, "1"]
train_model <- bind_cols(train_num, train_cat) %>% mutate(stroke = train$stroke)
test_model  <- bind_cols(test_num, test_cat) %>% mutate(stroke = test$stroke)
# 8) Baseline model: Logistic regression
ctrl <- trainControl(method="cv", number=5, classProbs=TRUE, summaryFunction=twoClassSummary, savePredictions="final")
set.seed(42)
# caret requires the positive class as the first level sometimes; ensure factor levels:
train_model$stroke <- relevel(train_model$stroke, ref = "Stroke")
test_model$stroke  <- relevel(test_model$stroke,  ref = "Stroke")
glm_fit <- train(stroke ~ ., data = train_model,
method="glm",
family="binomial",
metric="ROC",
trControl = ctrl)
print(glm_fit)
# Predictions on test
glm_prob <- predict(glm_fit, newdata = test_model, type = "prob")[, "1"]
# 2) Basic cleaning & type conversion
df <- fread("healthcare_dataset_stroke_data.csv") %>%
as_tibble() %>%
mutate(
age = as.numeric(age),
avg_glucose_level = as.numeric(avg_glucose_level),
bmi = as.numeric(bmi),
gender = as.factor(gender),
ever_married = as.factor(ever_married),
work_type = as.factor(work_type),
Residence_type = as.factor(Residence_type),
smoking_status = as.factor(smoking_status),
hypertension = as.factor(hypertension),
heart_disease = as.factor(heart_disease),
stroke = factor(stroke, levels = c(0, 1),
labels = c("NoStroke", "Stroke"))
)
# 3) Missingness overview
missing_table <- sapply(df, function(x) sum(is.na(x)))
print(missing_table)
# 4) Impute BMI median within group (example: median BMI overall or by gender)
# Here we use overall median (or you can use grouping)
bmi_median <- median(df$bmi, na.rm = TRUE)
df$bmi <- ifelse(is.na(df$bmi), bmi_median, df$bmi)
df$bmi <- as.numeric(df$bmi)
# 5) Feature engineering - (if needed)
# e.g., bucket age into categories (optional)
df <- df %>%
mutate(age_group = case_when(
age < 18 ~ "child",
age >= 18 & age < 35 ~ "young_adult",
age >= 35 & age < 60 ~ "adult",
age >= 60 ~ "senior"
)) %>% mutate(age_group = factor(age_group, levels=c("child","young_adult","adult","senior")))
# 6) Train-test split (stratified by stroke)
train_index <- createDataPartition(df$stroke, p = 0.8, list = FALSE)
train <- df[train_index, ]
test  <- df[-train_index, ]
# 7) Preprocessing via caret
# We'll one-hot encode categorical vars and center/scale numeric variables
prep_recipe <- preProcess(train %>% select(age, avg_glucose_level, bmi),
method = c("center","scale"))
train_num <- predict(prep_recipe, train %>% select(age, avg_glucose_level, bmi))
test_num  <- predict(prep_recipe, test %>% select(age, avg_glucose_level, bmi))
# Create model frames with dummies
dummies <- dummyVars(~ gender + ever_married + work_type + Residence_type + smoking_status + hypertension + heart_disease + age_group,
data = train)
train_cat <- predict(dummies, newdata = train) %>% as.data.frame()
test_cat  <- predict(dummies, newdata = test) %>% as.data.frame()
train_model <- bind_cols(train_num, train_cat) %>% mutate(stroke = train$stroke)
test_model  <- bind_cols(test_num, test_cat) %>% mutate(stroke = test$stroke)
# 8) Baseline model: Logistic regression
ctrl <- trainControl(method="cv", number=5, classProbs=TRUE, summaryFunction=twoClassSummary, savePredictions="final")
set.seed(42)
# caret requires the positive class as the first level sometimes; ensure factor levels:
train_model$stroke <- relevel(train_model$stroke, ref = "Stroke")
test_model$stroke  <- relevel(test_model$stroke,  ref = "Stroke")
glm_fit <- train(stroke ~ ., data = train_model,
method="glm",
family="binomial",
metric="ROC",
trControl = ctrl)
print(glm_fit)
# Predictions on test
glm_prob <- predict(glm_fit, newdata = test_model, type = "prob")[, "Stroke"]
glm_pred <- predict(glm_fit, newdata = test_model)
glm_roc <- roc(as.numeric(as.character(test_model$stroke)), glm_prob)
glm_roc <- roc(response = test_model$stroke,
predictor = glm_prob,
levels = c("NoStroke", "Stroke"))
cat("Logistic AUC:", auc(glm_roc), "\n")
conf_mat_glm <- confusionMatrix(glm_pred, test_model$stroke, positive = "Stroke")
print(conf_mat_glm)
# 9) Random Forest baseline
set.seed(42)
rf_fit <- train(stroke ~ ., data = train_model,
method = "ranger",
tuneLength = 5,
metric="ROC",
trControl = ctrl)
print(rf_fit)
rf_prob <- predict(rf_fit, newdata = test_model, type = "prob")[, "1"]
rf_prob <- predict(rf_fit, newdata = test_model, type = "prob")[, "1"]
rf_prob <- predict(rf_fit, newdata = test_model, type = "prob")[, "Stroke"]
rf_pred <- predict(rf_fit, newdata = test_model)
rf_roc <- roc(as.numeric(as.character(test_model$stroke)), rf_prob)
rf_roc <- roc(
response  = test_model$stroke,
predictor = rf_prob,
levels    = c("NoStroke", "Stroke")
)
cat("RF AUC:", auc(rf_roc), "\n")
print(confusionMatrix(rf_pred, test_model$stroke, positive="1"))
print(confusionMatrix(rf_pred, test_model$stroke, positive="Stroke"))
# 10) XGBoost (caret wrapper)
set.seed(42)
xgb_fit <- train(stroke ~ ., data = train_model,
method = "xgbTree",
tuneLength = 6,
metric="ROC",
trControl = ctrl)
print(xgb_fit)
xgb_fit <- train(stroke ~ ., data = train_model,
method = "xgbTree",
tuneLength = 6,
metric="ROC",
trControl = ctrl)
# for debugging
library(caret)
library(xgboost)
getModelInfo("xgbTree")$xgbTree$label
xgb_ctrl <- trainControl(
method = "cv",
number = 3,                 # fewer folds, faster for debugging
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = "final",
verboseIter = TRUE
)
xgb_fit <- try(
train(
stroke ~ .,
data = train_model,
method = "xgbTree",
tuneLength = 3,           # smaller grid for now
metric = "ROC",
trControl = xgb_ctrl
),
silent = FALSE
)
xgb_fit
print(xgb_fit)
xgb_prob <- predict(xgb_fit, newdata = test_model, type = "prob")[, "Stroke"]
xgb_pred <- predict(xgb_fit, newdata = test_model)
xgb_roc <- roc(as.numeric(as.character(test_model$stroke)), xgb_prob)
xgb_roc <- roc(response  = test_model$stroke, predictor = xgb_prob, levels    = c("NoStroke", "Stroke"))
cat("XGB AUC:", auc(xgb_roc), "\n")
print(confusionMatrix(xgb_pred, test_model$stroke, positive="Stroke"))
# 11) Plot ROC curves
roc_df <- data.frame(
glm = as.numeric(glm_prob),
rf = as.numeric(rf_prob),
xgb = as.numeric(xgb_prob),
true = as.numeric(as.character(test_model$stroke))
)
# Using pROC to plot
plot(glm_roc, main="ROC Curves - Models")
plot(rf_roc, add=TRUE, col=2)
plot(xgb_roc, add=TRUE, col=3)
legend("bottomright", legend=c(sprintf("Logistic AUC=%.3f", auc(glm_roc)),
sprintf("RF AUC=%.3f", auc(rf_roc)),
sprintf("XGB AUC=%.3f", auc(xgb_roc))),
col=1:3, lwd=2)
# 12) Save final model (example: xgboost)
saveRDS(xgb_fit, "xgb_fit_final.rds")
# Using pROC to plot
plot(glm_roc, main="ROC Curves - Models")
plot(rf_roc, add=TRUE, col=2)
plot(xgb_roc, add=TRUE, col=3)
legend("bottomright", legend=c(sprintf("Logistic AUC=%.3f", auc(glm_roc)),
sprintf("RF AUC=%.3f", auc(rf_roc)),
sprintf("XGB AUC=%.3f", auc(xgb_roc))),
col=1:3, lwd=2)
#                    "xgboost","randomForest","gbm","ROCR","ranger","patchwork"))
setwd("C:/Users/user/Documents/GitHub/Data_Science")
df <- fread("healthcare_dataset_stroke_data.csv") %>%
as_tibble()
cat("Rows:", nrow(df), "Columns:", ncol(df), "\n")
print(names(df))
print(table(df$stroke))
df <- df %>%
mutate(
age = as.numeric(age),
avg_glucose_level = as.numeric(avg_glucose_level),
bmi = as.numeric(bmi),
gender = as.factor(gender),
ever_married = as.factor(ever_married),
work_type = as.factor(work_type),
Residence_type = as.factor(Residence_type),
smoking_status = as.factor(smoking_status),
hypertension = as.factor(as.character(hypertension)),
heart_disease = as.factor(as.character(heart_disease)),
stroke = factor(stroke,
levels = c(0, 1),
labels = c("NoStroke", "Stroke"))
)
missing_table <- sapply(df, function(x) sum(is.na(x)))
print(missing_table)
bmi_median <- median(df$bmi, na.rm = TRUE)
df$bmi <- ifelse(is.na(df$bmi), bmi_median, df$bmi)
df$bmi <- as.numeric(df$bmi)
df <- df %>%
mutate(
age_group = case_when(
age < 18 ~ "child",
age >= 18 & age < 35 ~ "young_adult",
age >= 35 & age < 60 ~ "adult",
age >= 60 ~ "senior"
)
) %>%
mutate(age_group = factor(age_group,
levels = c("child","young_adult","adult","senior")))
train_index <- createDataPartition(df$stroke, p = 0.8, list = FALSE)
train <- df[train_index, ]
test  <- df[-train_index, ]
# Numeric scaling
prep_recipe <- preProcess(
train %>% select(age, avg_glucose_level, bmi),
method = c("center", "scale")
)
train_num <- predict(prep_recipe, train %>% select(age, avg_glucose_level, bmi))
test_num  <- predict(prep_recipe, test %>% select(age, avg_glucose_level, bmi))
# Categorical dummies
dummies <- dummyVars(
~ gender + ever_married + work_type + Residence_type +
smoking_status + hypertension + heart_disease + age_group,
data = train
)
train_cat <- predict(dummies, newdata = train) %>% as.data.frame()
test_cat  <- predict(dummies, newdata = test)  %>% as.data.frame()
# Final model matrices
train_model <- bind_cols(train_num, train_cat) %>%
mutate(stroke = train$stroke)
test_model  <- bind_cols(test_num, test_cat) %>%
mutate(stroke = test$stroke)
# Ensure positive class is "Stroke"
train_model$stroke <- relevel(train_model$stroke, ref = "Stroke")
test_model$stroke  <- relevel(test_model$stroke,  ref = "Stroke")
ctrl <- trainControl(
method = "cv",
number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = "final"
)
set.seed(42)
glm_fit <- train(
stroke ~ .,
data = train_model,
method = "glm",
family = "binomial",
metric = "ROC",
trControl = ctrl
)
print(glm_fit)
glm_prob <- predict(glm_fit, newdata = test_model, type = "prob")[, "Stroke"]
glm_pred <- predict(glm_fit, newdata = test_model)
glm_roc <- roc(
response  = test_model$stroke,
predictor = glm_prob,
levels    = c("NoStroke","Stroke")
)
cat("Logistic AUC:", auc(glm_roc), "\n")
conf_mat_glm <- confusionMatrix(glm_pred, test_model$stroke, positive = "Stroke")
print(conf_mat_glm)
rf_fit <- train(
stroke ~ .,
data = train_model,
method = "ranger",
tuneLength = 5,
metric = "ROC",
trControl = ctrl
)
print(rf_fit)
rf_prob <- predict(rf_fit, newdata = test_model, type = "prob")[, "Stroke"]
rf_pred <- predict(rf_fit, newdata = test_model)
rf_roc <- roc(
response  = test_model$stroke,
predictor = rf_prob,
levels    = c("NoStroke","Stroke")
)
cat("RF AUC:", auc(rf_roc), "\n")
conf_mat_rf <- confusionMatrix(rf_pred, test_model$stroke, positive = "Stroke")
print(conf_mat_rf)
set.seed(42)
knn_fit <- train(
stroke ~ .,
data = train_model,
method = "knn",
tuneLength = 10,
metric = "ROC",
trControl = ctrl
)
print(knn_fit)
knn_prob <- predict(knn_fit, newdata = test_model, type = "prob")[, "Stroke"]
knn_pred <- predict(knn_fit, newdata = test_model)
knn_roc <- roc(
response  = test_model$stroke,
predictor = knn_prob,
levels    = c("NoStroke","Stroke")
)
cat("kNN AUC:", auc(knn_roc), "\n")
conf_mat_knn <- confusionMatrix(knn_pred, test_model$stroke, positive = "Stroke")
print(conf_mat_knn)
set.seed(42)
gbm_fit <- train(
stroke ~ .,
data = train_model,
method = "gbm",
tuneLength = 5,
metric = "ROC",
trControl = ctrl,
verbose = FALSE
)
print(gbm_fit)
gbm_prob <- predict(gbm_fit, newdata = test_model, type = "prob")[, "Stroke"]
gbm_pred <- predict(gbm_fit, newdata = test_model)
gbm_roc <- roc(
response  = test_model$stroke,
predictor = gbm_prob,
levels    = c("NoStroke","Stroke")
)
cat("GBM AUC:", auc(gbm_roc), "\n")
conf_mat_gbm <- confusionMatrix(gbm_pred, test_model$stroke, positive = "Stroke")
print(conf_mat_gbm)
set.seed(42)
xgb_fit <- train(
stroke ~ .,
data = train_model,
method = "xgbTree",
tuneLength = 3,      # can increase for more tuning if time allows
metric = "ROC",
trControl = ctrl
)
print(xgb_fit)
xgb_prob <- predict(xgb_fit, newdata = test_model, type = "prob")[, "Stroke"]
xgb_pred <- predict(xgb_fit, newdata = test_model)
xgb_roc <- roc(
response  = test_model$stroke,
predictor = xgb_prob,
levels    = c("NoStroke","Stroke")
)
cat("XGB AUC:", auc(xgb_roc), "\n")
conf_mat_xgb <- confusionMatrix(xgb_pred, test_model$stroke, positive = "Stroke")
print(conf_mat_xgb)
# Optional data frame for later analysis
roc_df <- data.frame(
glm  = as.numeric(glm_prob),
rf   = as.numeric(rf_prob),
knn  = as.numeric(knn_prob),
gbm  = as.numeric(gbm_prob),
xgb  = as.numeric(xgb_prob),
true = ifelse(test_model$stroke == "Stroke", 1, 0)
)
plot(glm_roc, main = "ROC Curves - All Models")
plot(rf_roc,  add = TRUE, col = 2)
plot(knn_roc, add = TRUE, col = 3)
plot(gbm_roc, add = TRUE, col = 4)
legend(
"bottomright",
legend = c(
sprintf("Logistic AUC=%.3f", auc(glm_roc)),
sprintf("RF AUC=%.3f",      auc(rf_roc)),
sprintf("kNN AUC=%.3f",     auc(knn_roc)),
sprintf("GBM AUC=%.3f",     auc(gbm_roc)),
sprintf("XGB AUC=%.3f",     auc(xgb_roc))
),
col = 1:5,
lwd = 2,
cex = 0.8
)
saveRDS(xgb_fit, "xgb_fit_final.rds")
plot(xgb_roc, add = TRUE, col = 5)
